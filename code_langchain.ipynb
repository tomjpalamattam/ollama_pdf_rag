{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7fd2087-b002-454d-af7c-711be8f0eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def convert_files_to_txt(src_dir, dst_dir):\n",
    "    # If the destination directory does not exist, create it.\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    \n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.jpg'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Get the relative path to preserve directory structure\n",
    "                rel_path = os.path.relpath(file_path, src_dir)\n",
    "                \n",
    "                # Create the same directory structure in the new directory\n",
    "                new_root = os.path.join(dst_dir, os.path.dirname(rel_path))\n",
    "                os.makedirs(new_root, exist_ok=True)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = f.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                            data = f.read()\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Failed to decode the file: {file_path}\")\n",
    "                        continue\n",
    "                \n",
    "                # Create a new file path with .txt extension\n",
    "                new_file_path = os.path.join(new_root, file + '.txt')\n",
    "                with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(data)\n",
    "\n",
    "# Call the function with the source and destination directory paths\n",
    "convert_files_to_txt('chi21adaptive', 'converted_codebase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d410316-b435-491d-be66-c6b91e10ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chi21adaptive'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
      "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
      "remote: Total 70 (delta 17), reused 53 (delta 10), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (70/70), 370.94 KiB | 2.36 MiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aalto-ui/chi21adaptive.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4beac09a-9089-4606-97e5-931995f01493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████▉                        | 50/78 [00:00<00:00, 5575.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 50\n",
      "Number of documents : 440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "src_dir = \"converted_codebase\"\n",
    "loader = DirectoryLoader(src_dir, show_progress=True, loader_cls=TextLoader)\n",
    "repo_files = loader.load()\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9a8502-6bb6-43ea-b0f4-81254102d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    " old_path_with_txt_extension = doc.metadata[\"source\"]\n",
    " new_path_without_txt_extension = old_path_with_txt_extension.replace(\".txt\", \"\")\n",
    " doc.metadata.update({\"source\": new_path_without_txt_extension})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b3148e-d7a3-43f9-87ab-3aaf18439301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7ec3dd-1dc3-4939-b1b5-bf2590e27518",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=model_name,\n",
    " show_progress=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d4c188-c1b8-4df7-a27b-dcef453a28c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/WD/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/tom/WD/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\":True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_name,\n",
    " model_kwargs=model_kwargs,\n",
    " encode_kwargs=encode_kwargs,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737c172-ff65-4748-b266-cab370070ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont execute\n",
    "\n",
    "# Add documents to vector database in addition to persistant directory (execute this or the one below)\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding= embeddings,\n",
    "    collection_name=\"local-rag\",\n",
    "    persist_directory=\"./chroma_db_pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df796b2e-f865-448a-a13f-72f0c15aa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    " documents,\n",
    " embeddings,\n",
    " path=\"local_qdrant\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bdf038-4759-4f9d-bde3-a07e99e509ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(documents):\n",
    "    for doc in documents:\n",
    "        print(doc.metadata)\n",
    "        print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")\n",
    "        print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "015a0f95-f455-426a-9383-169ef68608c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats policy network here?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "#pretty_print_docs(found_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c901a8-3d39-4f9d-98cf-0226744f7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"llama3\"\n",
    "llm = ChatOllama(model=local_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481e537-bee0-497e-b2aa-fe500869c0f8",
   "metadata": {},
   "source": [
    "### Implementation 1 (directly from serach docs and not from retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea11517-a1d1-48fa-8d67-1758b947009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, Policy Network refers to a model design for predicting which menu states are worth of expansion in MCTS. The policy network is implemented as an artificial neural network with three heads and one tail. Its input includes source menu configuration, click distributions, and association matrix between menu elements.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\":query,\"context\":found_docs})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ae152-cdf6-417c-9de7-1abf6d19d67e",
   "metadata": {},
   "source": [
    "### Implementation 2 (using retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74bdfefc-a2df-4363-875b-a96e903a2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd0c279-b7b2-4c66-994f-0a0ac6121dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    qdrant.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814ef723-18a7-456f-bd6a-9b2f1c3e7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define templates for prompts\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#Initialte chat_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Create a memory instance\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\", memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Define steps for the chain\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define templates for prompts\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"HumanMessage: \" + dialogue_turn[0]\n",
    "        ai = \"AIMessage: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],        \n",
    "#        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n",
    "# Create the final chain by combining the steps\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c74972b2-dbbc-433f-b8c3-492e36953041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like I've been dropped into a rather complex task!\n",
      "\n",
      "Based on the provided context and code snippet, it appears that I'm supposed to help with training a policy network model for MCTS (Monte Carlo Tree Search). This model is used in some kind of decision-making process, likely related to menu adaptation.\n",
      "\n",
      "To train this model, we need to generate some training data using the \"pump\" program. The generated data should be stored in a file named `results.txt`, with each line following a specific format:\n",
      "\n",
      "```\n",
      "[serial,forage,recall][source_menu][source_frequencies][source_associations][target_menu][target_frequencies][target_associations][exposed]\n",
      "```\n",
      "\n",
      "This format seems to contain information about the source menu configuration (elements, click distribution, and association matrix), the target menu configuration (elements, click distribution, and association matrix), and whether the exposed flag is True or False.\n",
      "\n",
      "Please let me know if there's anything else I can help with!"
     ]
    }
   ],
   "source": [
    "#stream chain 7\n",
    "\n",
    "input = \"\"\"\n",
    "whats the policy network for?\n",
    "\"\"\"\n",
    "inputs = {\"question\": input, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "chunks = []\n",
    "chunks_answer = []\n",
    "for chunk in final_chain.stream(inputs):\n",
    "    chunks.append(chunk)\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk['answer'].content, end='')\n",
    "        chunks_answer.append(chunk['answer'].content)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Below code might not work\n",
    "\n",
    "\n",
    "# Save the conversation in memory\n",
    "#generated_answer = chunks['answer']\n",
    "\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=input),\n",
    "    AIMessage(content=chunks_answer),\n",
    "    #AIMessage(content=result[\"answer\"].content),\n",
    "])\n",
    "\n",
    "\n",
    "# Load memory to see the conversation history\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "#memory.save_context(inputs, {\"answer\": generated_answer.content})\n",
    "memory.save_context(inputs, {\"answer\": chunks_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202854e6-6bf9-4172-a9f4-e2e213edd92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
